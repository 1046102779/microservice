Jaeger的RPC框架为TChannel，它是基于Thrift协议，虽然现在市面上的RPC产品大多采用Google GRPC，uber jaeger官方说了，未来会考虑增加grpc的支持，但是不是太紧急.


## TChannel设计目标

TChannel设计目标，共六点：

1. 多语言支持
2. 高性能地快速决策路由转发
3. 请求和响应的无序性，以便慢速请求不会阻止后续更快的请求
4. 可选的校验和
5. 能够在endpoints之间支持多种协议的数据传输（例如：HTTP+JSON和Thrift）


首先我们了解下TChannel的官方文档：

## protocol

在微服务中，设计RPC服务框架时，经常遇到的问题，三个：

1. 服务注册与服务发现—— 我怎么发现服务端有哪些服务，以及哪些服务是可用的？
2. 容错性—— 当RPC服务框架的server出现问题时，怎么迅速隔离？另一个问题，如果当客户端调用服务端发生错误时，是立即抛异常，还是进行容忍，再次尝试等
3. 加入追踪能力Dapper—— 如何识别和监控整个系统的瓶颈？

对于分布式追踪系统的整体概貌，我觉得有个京东金融的视频讲得很好，大家可以认真看看

[京东金融分布式服务跟踪实践](http://www.infoq.com/cn/presentations/financial-distributed-service-tracking-in-jingdong)

###  设计目标

TChannel目标就是解决这三个问题，提供一个智能路由协议来让client发现可用的server的方案，例如：`Hyperbahn`

再次考虑微服务面临的三个问题：

1. 服务注册与服务发现：所有的服务通过路由网格进行服务注册。消费者通过服务名找到服务，它无需知道IP和端口，也就是通过服务名，就可以访问server
2. 容错性：路由网格能够跟踪并计算一些指标，如：失败率，服务正常可用时间等。它能够智能地检测服务的健康状况，并从可用的server列表中移除它，并采用策略尝试添加回来；
3. request跟踪：trace跟踪是在rpc框架设计中必须优先考虑的

首先，在考虑设计微服务时，我们需要整合client和server之间的逻辑，并推送核心特性到微服务架构中，这样应用就再无需升级更新库

```shell
Consolidating logic between producers and consumers also allows us to push core features to our entire SOA without requiring applications to update libraries.
```

另外，在开发这个协议和路由网格时，我们要记住并遵循以下目标：

1. 这个协议必须是支持多语言实现的，并且这样做是很容易的。特别是JavaScript、Python和Go。
2. 异步行为是最基本的要求，我们遵循传统的request/response模型，且这个模型支持无序响应，不阻塞的。慢请求一定不能阻止子请求；
3. 大的request/response可能或者一定要被分割成多个片段发送(例如：支持`streaming`请求)
4. TChannel能够通过任意的序列化方案。例如：JSON、Thrift。其中，Thrift是IDL，类型安全和验证保证的首选。
5. 路由网格需要一个高性能转发路径，它能够快速做出路由决策，选择最合适的路由。
6. request/response集成的可选校验和

### 字段长度约束

在这个协议中，所有数字值都是无符号的和大端的，并且所有的字符串都是UTF-8。所有的键值对在http header中都是字符串。

下面描述字段长度和统计和finagle-mux是一样的schema：

1. `size:4 body:10`定义了这个field大小是4个字节，跟着就是body字段大小为10个字节。
2. `key~4`表示这个`key`有4个字节，跟着就是数据流
3. `key~4`是`keysize:4 key:keysize`的简写

Groups有括号组成，对于多次重复，group的重复计数为{*}, 或者对于n次重复，则为{n}。

### 消息流

TChannel是一个双向流的request/response协议。在peers之间，无论哪一方发起的建立请求，连接都是等效的。甚至可能希望在同一对peers之间具有多个连接。消息ID列表限定在一个连接上（TODO：不太理解）。发送一个请求到一个连接上，并从另一个连接上响应，这是无效的。无论出于何种原因，可以将一个请求发送到一个连接上，然后将一个后续请求发送到另一个连接上。

`ps: 上面的意思是，TChannel建立了Client/Server的多个TCP连接，如果A Client发送请求到a连接上， 则Server对A的响应，一定是走a连接回路。至于后续的请求，对于业务方认为是一次新的连接建立，随便选择连接`

一个消息对应共享ID的所有帧。

1. 一对"init req"和"init res"帧共享同一个消息ID ；
2. 对于"call req", "call req continue", "call res", "call res continue"和"errors"帧可以共享同一个消息ID。
3. 一对"ping req"和"ping res"帧共享同一个消息ID。

初始化一个新的连接步骤：

1. node A初始化一个TCP连接到node B。 client：node A；server：node B
2. node B建立了TCP连接，但是要等到node A发送消息"init req"时，node B才会发送消息给A；
3. node A发送想要版本号的消息"init req"，直到node B响应消息"init res"之前都不会再发送任何消息；
4. node B发送带着选中的版本V消息"init res"，node B现在可以发送带有版本V的请求；
5. node A接收到消息"init res"，node A现在可以发送带着版本V的请求

每一个消息都被封装在帧中，这个帧处理业务数据本身还有额外通用的附加信息。帧信息的一部分是一个ID， 当发送请求消息时，请求者选择此ID。当响应到一个请求时，这个响应节点使用请求发过来的消息ID。

每一帧都有一个类型，它描述了这个帧的body格式。依赖于这个帧类型，有些body是无数据的。具体如下所示：

```shell
消息为"call req"的帧类型有个字段是"ttl"，它用于表示这个请求的生命周期。transmitter管理这个ttl以考虑传输花费时间或者故障转移时间，仅仅在（deadline propagation purposes 截止传播...）发送这个ttl给接收者。这个允许接收者知道他们还有多少时间来完成这个请求的剩余数据传输。只要ttl不为0，则表示这个请求还可以继续完成或者重新传输。
```

一个服务实例A1，通过服务路由R1，进入到服务实例B1，然后返回。这里有一些消息流的细节，下面的这些不是每个字段的完整细节，但是他们可以说明一些不太明显的行为。

这个例子中的消息流：`A1 ——> R1 ——> B1 ——> R1 ——> A1`

A1发送消息为"call req"(消息类型:0x03)给R1：

1. 为这个帧选择消息ID：M1。这个将用于匹配此请求的响应；M1的范围在A1和R1之间，以及这个连接上；
2. 生成一个唯一traceid，这个traceid可以传播到任意依赖的请求中。且只有在调用链中最边缘的服务时才应该这样做。(`ps: 这个就是调用链发起的地方，也即开端`)。这个唯一traceid目的是创建一个span树，并覆盖所有RPCs
3. 生成一个唯一的spanid，表示当前节点处理的工作
4. 为这个请求设置一个最大允许时间ttl。如果在这个时间内，响应端没有返回数据，则client节点会取消这个请求
5. 服务A1不需要任何头部，因此默认也是这样使用的
6. 服务A1发起带有服务B1的请求，消息体"call req", 在服务B1需要进行数据校验，防篡改


R1接收到来自服务A1的"call req"消息，发送"call req"(0x03)给服务B1：

1. 为这个帧选择消息ID：M2；
2. 拷贝来自流入的消息traceid到新消息的traceid部分；
3. 拷贝来自流入的消息spanid到新消息的parentid部分；
4. 生成唯一的spanid
5. 拷贝来自流入的消息ttl到新的消息ttl部分
6. 拷贝服务名、参数和校验数据到新的消息

B1接收来自R1的消息"call req", 发送"call res"(0x04)给R1：

1. 为这个帧使用消息ID：M2；
2. 发送来自应用响应的参数和校验和

R1接收来自B1的消息"call res"， 发送"call res"(0x04)给A1：

1. 匹配流入的消息ID：M2，消息内容为"call req";
2. 为新消息使用消息ID：M1；
3. 拷贝流入的消息参数和校验和；

A1接收来自R1的消息"call res":

1. 匹配流入的消息ID：M1，且已存在的消息为"call req"
2. 发送参数给应用程序；

### 帧

所有类型的帧统一使用如下的结构表示：(这里的数字单位：字节)

|Position | 内容 |
|---|---|
| 0-7 | `size:2 type:1 reserved:1 id:4` |
| 8-15 | `reserved:8` |
| 16+ | `payload:` - base on type |


**size:2**

size:2表示这个帧的最大范围2个字节：64kb-1，包括帧的头部和数据部分。注意尽管有些其他字段也是指定16个比特大小。`实现时必须注意不能超过这个帧的大小值：64kb-1`

**type:1**

数据负载类型，有效类型如下所示：

| code | 类型名称 | 描述 |
|---|---|---|
| `0x01` | init req | 建立连接的第一个消息 |
| `0x02` | init res | init req消息的响应 |
| `0x03` | call req | RPC方法的请求 |
| `0x04` | call res | RPC方法的响应 |
| `0x13` | call req continue | RPC请求的分片 |
| `0x14` | call res continue | RPC响应的分片 |
| `0xc0` | cancel | 取消未完全的呼叫请求/转发请求（body为空） |
| `0xc1` | claim | 声明/取消 冗余请求 |
| `0xd0` | ping req | 这个用于健康检查 (body为空)| 
| `0xd1` | ping res | 响应 （body为空） |
| `0xff` | error | TChannel框架内部错误 |
 
成帧层对所有有效载荷都是通用的。它有意图的限制帧大小为64kb, 以便允许跨共享TCP连接更好地交错帧(interleaving of frames)。为了处理更多的操作实现，这将需要解码payload有效载荷的上层协议

**reserved:1**

预留1个字节，作为未来协议的扩展，目前暂不用

**id:4**
 
消息ID占4个字节，它由请求的发送端选择，然后响应时返回同样的消息ID。这个消息ID仅仅是对于这个发送者有效的。这类似于TCP在每个方向上具有序列号的方式。连接的每一端可能发生选择消息ID重叠现象，这个是ok的，因为他们是具有方向的 

`id`代表最顶端的消息ID。请求帧和响应帧都是相同消息ID，用于消息匹配。在请求被分解为片段序列之后，可以在多个请求或者响应帧上使用单个消息ID，如下所述：

消息ID的有效值：`0`到`0xFFFFFFFE`。这个`0xFFFFFFFF`值被保留，用于协议错误响应

**reserved:8**

将来用于协议扩展，暂不用

**payload**

由协议帧的type字段决定body的0~N个字节内容。payload的长度等于总帧大小-16字节

有关每一种payload, 具体详见下文


### Payloads

#### init req(0x01)

schema: `version: 2 nh: 2 (key~2 value~2){nh}`

这个一定是建立连接的第一个消息。它用于协商通用协议版本和描述两端的服务名称。将来，我们可能使用这个来协商身份认证和授权。

**version**

`version`是占用两个字节的数字。当前这个指定版本协议为2.如果新版本被要求，一个常用版本也是可以协商的。

**headers**

这里有些键值对。对于版本号是2的版本，下面是必须要求的：

| 名称 | 格式 | 描述 | 
| --- | --- | --- |
| `host_port` | `address:port` | remote server |
| `process_name` | 任意字符串 | 这个实例的附加信息，用于日志 |
| `tchannel_language` |  任意字符串 | 例如："node", "python", "go" |
| `tchannel_language_version` | 任意字符串 | 语言版本号 |
| `tchannel_version` | 任意字符串 | 当前使用的tchannel版本号 | 

后面四个参数都是指调用方，也就是client。

考虑到向后兼容，实现应该忽略这五个之外的key-value列表

对于无法监听新连接或者无意义的连接，实现应发送`host_port`: `0.0.0.0`。此特殊值告诉接收实现使用getpeername(2) 或者等效API的结果来唯一标识此连接。它还告诉接收者这个连接之外的地址是无效的，因此不应该将其转发到其他节点。

#### init res(0x02)

schema: `version:2 nh:2 (key~2 value~2){nh}`

请求建立时，client request选择了一个版本号，那么server响应时也会使用这个版本号。这个key-values键值对同上

#### call req(0x03)

schema: 

```shell
flags: 1 ttl: 4 tracing: 25
service~1 nh: 1(hk~1 hv~1){nh}
csumtype: 1(csum:4){0, 1} arg1~2 arg2~2 arg3~2
```

这个"call req" 是最主要的RPC机制，元组`{arg1, arg2, arg3|`在连接建立之后通过数据传输发送到server中

不管是client直接连接到server，还是通过router连接到的server，这个服务名总是被指定的。这个支持显式路由模型以及选择将某些请求通过router委托给另一个服务，这两者是等同的。

路由转发可以在不理解消息体中body的情况下，router payload。

一个"call req"可以被分成多个帧。这样第一个帧的type为”call req“，接下来在其他帧叫做"call req continue"

**flags:1**

使用控制片段指令，有效值：

| flag | 描述 |
|--- | --- |
| `0x01` | 多帧片段
| `0x02` | 是请求流

如果flag没有设置，它表示消息ID是唯一或者最后一帧

如果flag设置为`0x02`, 表示流请求超时语义(到第一响应帧的时间)而不是非流语义(时间到最后响应帧)。(`原文：If the streaming flag is set, then streaming request timeout semantics (time to first response frame) apply instead of the non-streaming semantics (time to last response frame).`)

flag=`0x02`, 这个帧只能是`CallRequest`或者`CallResponse`帧。如果这个帧设为`CallRequestCont`或者`CallResponseCont`帧，这是一个协议错误和无效Cont帧。（Cont：continue）

**ttl:4**

帧的生成时间(Time To Live), 单位：毫秒。路由中转时应该考虑酌情减少这个值。`在tcp中，ttl表示路由跳转数，每经过一个路由ttl减一`, 这个ttl最小为0。当ttl为0时，则这个请求永远不会被发送出去，同时生成一个error响应

**tracing:25**

Tracing有效负载，详见tracing部分

**service~1**

UTF-8字符串定义了应该被路由到的目标服务名，占用1字节

**nh:1**

传输头部，在"Transport Headers"部分详细描述

**csumtype:1(csum:4){0,1}**

Checksum详见"checksums"部分

**arg1~2 arg2~2 arg3~2**

这三个args的含义取决于每一端的系统。arg1, arg2和arg3的格式没有被指定，且每个arg都占用两个字节。 就TChannel而言，这些事不透明的二进制blob。

这个`arg1`最大值可表示16kb

未来版本可能允许callers指定具体的服务实例，去执行这个请求，或者将一定比例的所有流量路由到实例子集以进行canary分析的机制。

**call res(0x04)**

schema:

```shell
flags:1 code:1 tracing:25
nh:1 (hk~1 hv~1){nh}
csumtype:1 (csum:4){0,1} arg1~2 arg2~2 arg3~2
```

这个与`call req(0x03)`类似，不同在于：

1. 增加了`code`字段
2. 没有`ttl`字段
3. 没有`service`字段

所有公共字段与"call req"都有相同的定义，请参阅上面的详细信息部分。对于arg1来说，"call req"与"call res"具有相同的值是没有必要的；按照惯例，现有的实现将arg1保留以用于"call res"消息

`arg1`最大值为16kb

**code:1**

响应码：

| code | 名称 | 描述 |
| --- | --- | --- |
| `0x00` | OK | 一切都是ok的
| `0x01` | Error | 应用程序错误，详见args

code为非零值没有暗示这个请求是否需要重试

实现应该用unix样式的零/非零逻辑，以便将来对其他"not ok"代码安全

#### cancel(0xC0)

schema: `ttl:4 tracing:25 why~2`

这个消息强制对一个"call req"的原响应，必须带有一个error类型`0x02`

这个cancel消息的ID必须能够匹配"call req"帧。

注意，这个消息IDs作用于一个连接上，cancel消息可能会出发一个或者多个依赖的消息发生cancel。

`why`字段用于描述为何cancel，仅仅用于日志记录

应该要注意的是，response和cancellation消息可以在连接中相互传递，因此我们需要能够在cancel后处理响应。我们还需要能够处于类似的原因处理重复的响应。如有必要，该网络的边缘需要实施自己的重复数据删除策略。

#### call req continue(0x13)

schema: `flags:1 csumtype:1 (csum:4){0,1}{continuation})`

这个帧继续"call req"。具体描述见"fragmentation"部分

"flags"与"call req"类型有相同定义： 控制flag

#### call res continue(0x14)

schema: `csumtype:1 (csum:4){0, 1} {continuation}`

这个帧继续一个"call res",具体见"fragmentation"部分

#### claim(0xC1)

schema: `ttl:4 tracing:25`

此消息用于声明或者取消冗余请求。当请求被发送到多个节点，它开始工作或者工作完成时，根据选项，他们将告诉其他节点关，以减少额外的工作。此声明消息从工作节点到其他工作节点。这个声明的请求由其完整的zipkin跟踪数据引用，该数据由第一个请求的发起者选择。

当一个节点B接收一个来自节点A的claim消息，这个消息带有节点B不知道的tracer T。B在短时间内将会注意到这个。如果节点B接收到带有trace T的请求延迟了，那么节点B会静默的忽略tracer T。这个是对于大多数都是期望的，因为在发送给A之后，且在发送给B之前，路由将会增加一些延时。

在Google的"The Tail at Scale"论文中有实现，他们描述的"backup request with cross-server cancellation", 这个演讲的ppt在[这里](http://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/Berkeley-Latency-Mar2012.pdf)

在page 39页，相关的视频在[这里](http://youtu.be/C_PxVdQmfpk?t=26m45s)

这个claim不太理解，后续再看 ::TODO

#### ping req(0xD0)

这个消息类型用于验证协议在连接上是否正常运行。接收方将发回"ping res"消息，但是预计应用程序不会看到此消息。如果需要更加详细的健康检查和验证检查，可以使用"call req"和"clal res"消息在更高级别实施这些检查

此消息类型没有正文

#### ping res(0xD1)

总是对"ping req"的一个回应消息。发送这个消息并不意味这服务就是健康的。它仅仅是验证网络的连通性和协议的正常性

这个消息类型没有正文

#### error(0xFF)

schema: `code:1 tracing:25 message~2`

在这个协议上或者系统因为某种原因无法触发RPC请求时，响应一个失败消息。应用级别的错误不再这里体现，这里的error是指RPC框架内部的错误。 应用或者业务的错误是保存在"call res"消息内指定的args异常数据中。

帧头部的消息ID应该是原始请求的消息ID，如果没有消息ID可用，则消息ID设置为`0xFFFFFFFF`

**code:1**

内部响应码：

| code | 名称 | 描述 |
| --- | --- | --- |
| `0x00` | invalid | 不使用 |
| `0x01` | timeout | 超时 | 
| `0x02` | cancelled | 带有cancel消息 |
| `0x03` | busy | 节点忙，如果可以的话，这个请求可以进行重试 |
| `0x04` | declined | 因为某种原因拒绝（比如：负载、流控、黑名单等）| 
| `0x05` | unexpected error | 请求结果发生了异常错误 | 
| `0x06` | bad request | 请求参数不匹配 | 
| `0x07` | network error | 网络错误 |
| `0x08` | unhealthy | 节点不健康 |
| `0xFF` | fatal protocol error | 协议不支持， 消息ID为`0xFFFFFFFF` | 

**tracing:25**

Tracing payload, 见tracing部分。

**message~2**

这个消息不打算向用户展示。他面向错误日志用以帮助工程师调试日志和错误。

### Tracing

schema: `spanid:8 parentid:8 traceid:8 traceflags:1`

tracing总字节数25个。

| 字段名 | 类型 | 描述 |
| --- | --- | --- |
| `spanid` | int64 | 表示当前span | 
| `parentid` | int64 | 当前span的父级spanid |
| `traceid` | int64 | 原始请求traceid，也即调用链开端生成的跟踪id |
| `traceflags` | uint8 | bit flags字段 |

一个*span*是一个逻辑操作，如：call或者forward

这个`traceid`是一个完整调用链的追踪，它在上下文传播过程中始终保持不变的，无论经过多少服务

当一个请求进入到我们的系统时，且这个请求的traceid等于0，则需要生成一个全局唯一的traceid，同时生成一个spanid，如果节点处理请求的client带有span，则这个节点的parentid为上一个spanid。

Trace flags:

| flag | 描述 |
| --- | --- |
| `0x01` | 为此请求开启trace |

当这个trace flag没有设置时，这个跟踪数据仍然要求在这个帧出现，但是这个调用链数据将不会再Zipkin存储中出现

### Transport Headers

在传输和路由上，这些headers意图控制一些东西，因此，期望付出的成本小。应用级别的headers是在协议的高层封装的，也就是payload中封装自己的业务数据协议。

重复key是不允许的，且应该爆出一个解析错误

Header键不可以为0，但是header值可以为0

Header键有一个最大长度16字节；这个传输header的总数量最大允许128个键值对

schema: `nh:1 (hk~1 hv~1){nh}`

TChannel headers有0~N个键值对，数据类型都是`UTF-8`的字符串。

这个headers的数量填充到协议的第一个比特(`nh`)中

如果`nh`为0，则表示这个头部没有比特

如果`nh`大于等于1，则表示有多个key/value键值对

每一个键值对字符串前面都有一个字节编码其长度。

例如，下面的hex dump：

```shell
0103 6369 6402 6869 ..cid.hi
```

编码后的键值对：`("cid", "hi")`

下面的表列出了有效传输header键，无论他们在"call req"或者"call res"中是有效还是无效的。下面详细阐述：

| name | req | res | 描述 |
| --- | --- | --- | --- |
| `as` | Y | Y | arg schema |
| `cas` | Y | N | Claim At Start |
| `caf` | Y | N | Claim At Finish |
| `cn` | Y | N | Caller Name |
| `re` | Y | N | Retry Flags |
| `se` | Y | N | Speculative Execution |
| `fd` | Y | Y | Failure Domain |
| `sk` | Y | N | Shard key |
| `rd` | Y | N | Routing Delegate |


#### Transport Header |  (`as` —— Arg Scheme)

这个header `as`是必须的

这描述了端点处理程序/协议检查的args格式。这个最主要的RPC机制是使用"thrift", "http"和"json"被用于和其他系统的交互上。

下面表格描述`as`值和响应arg1, arg2和arg3形式：

1. 对于`as=thrift` ,详见 [thrift arg scheme definition](https://github.com/uber/tchannel/blob/master/docs/thrift.md)
2. 对于`as=sthrift`, 详见 [streaming thrift arg scheme definition](https://github.com/uber/tchannel/blob/master/docs/sthrift.md)
3. 对于`as=json`, 详见 [json arg scheme definition](https://github.com/uber/tchannel/blob/master/docs/json.md)
4. 对于`as=http`, 详见 [http arg scheme definition](https://github.com/uber/tchannel/blob/master/docs/http.md)
5. 对于`as=raw`, 详见 [raw arg scheme definition](https://github.com/uber/tchannel/blob/master/docs/raw.md)

#### Transport Header | (`cas` —— Claim At Start)

值为字符串 "host:port"

这个请求也被发送到另一个端口为`host:port`的实例。当工作开始时发送一个claim消息。

#### Trasport Header | (`caf` —— Claim At Finish)

值为字符串 "host:port"

当response被发送时，发送claim消息到端口为`host:port`的服务

#### Transport Header | ( `cn` —— Call Name)

这个`cn`是必须的

值是调用者的服务名。

#### Transport Header | (`re` —— Retry Flags)

Flags是用UTF-8编码的。每个Flag都是有单个字符表示。

| flag | 含义 |
| --- | --- |
| `n` | 不重试，直接暴露error错误. cancels 为 `c`和`t` |
| `c` | 连接错误时重试。这个具体重试机制没有被TChannel指定。实际的重试是由路由层处理 |
| `t` | 超时重试 |

对于`re`的有效值如下：

| value | 描述 |
| --- | --- |
| `c` | 默认为连接错误时，重试 |
| `t` | 超时重试 |
| `n` | 不重试 |
| `ct` | 连接错误或者超时的重试 |
| `tc` | 连接错误或者超时的重试 |

#### Transport Header | (`se` —— Speculative Execution)

推测执行次数，编码为单个数字(<10). 表示运行请求的节点数

此版本协议中`se`的唯一有效值是2.

将来我们可以将这个推测执行系统扩展到2个以上的节点。为了简化和最大限度地减少混淆，我们有意将`se`限制为2.

#### Transport Header | (`fd` —— Failure Domain)

描述对同一服务的一组相关请求的字符串，如果他们失败，则可能以相同的方式失败。

例如，某些请求可能具有对另一个服务的下游依赖，而其他请求可能完全在所请求的服务内处理。这用于实现类似Hystrix的"断路器"行为。

#### Transport Header | (`sk` —— Shard Key)

这个Shard键确定这个"call req"来自哪里。如果你想要一个TChannel环，请求转到特定节点，你可以设置一个Shard键header

ringpop使用此`sk`头，将"call req"传递给特定的TChannel实例。

例如，你可以希望保留一些内存状态，即缓存，聚合。你可以使用`sk`来读取并将调用请求转发给具有该分片键所有权的特定进程

#### Transport Header | (`rd` —— Routeing Delegate)

这个路由委托请求头必须设置为一个有效的服务名

当路由委托头部设置为我们将要路由的服务名，去替代call request的服务名

我们期望在`rd`头的服务名，能够让我们正确地找到目标服务


#### 有关`host:port`的注意事项

这些`host:port`字段全部都是string类型，它们可以提供其他未直连的实体可以回到正确的地址。

`原文：While these host:port fields are indeed strings, the intention is to provide the address where other entities not directly connected can reach back to this exact entity.`

在IPv4协议中，这些`host:port`字段应该是IPv4地址。类似于`127.0.0.1:8080`

这个字段不应该使用hostname或者DNS名称，除非我们找到令人信服的理由来使用它们。

例子：

1. `10.0.0.1:12345` —— IPv4 host:port
2. `[1fff:0:a88:8583:ac1f]:8001` —— IPv6 host:port


注意，IPv6地址在地址部分也有冒号，因此实现应使用字符串中的最后一个冒号将地址和端口分开。

### Deadline与TTL

这个TTL是指调用服务完成一次调用的允许最大等待时间。一个实例只要满足deadline，就可以重试。在上下文传播时，当调用链达到下游节点后，TTL需要减掉这个请求已经耗费掉的时间。为网络延添加一些属性可以帮助检测网络节点的水位情况。

TTLs不会在响应时带上。它是有调用方跟踪整个连接的时间消耗，并在分发新的子请求之间验证新的deadline。

### Checksums

为了简化在不同语言不同平台下的实现，Checksums是可选的。当TCP在连接内提供了checksum，TChannel payloads会意图进行多TCP连接传输，增加并发量。通过在源头进行checksum，中转和目的地都能够进行payload校验，防止信息丢失或者篡改。验证这些checksums和拒绝无效帧都是期望的，但不是必须的。

Checksums通过三个参数进行计算，如下所示：

```shell
csum = func(arg1, 0);
csum = func(arg2, csum);
csum = func(arg3, csum);
```

当消息被分片时，这个行为会有轻微的变化。具体见"fragmentation"

#### Checksum类型：

| `type:1` | scheme | 值长度 |
| --- | --- | --- |
| `0x00` | none | 0 bytes |
| `0x01` | crc-32 | 4 bytes |
| `0x02` | farmhash Fingerprint32 | 4 bytes |
| `0x03` | crc-32C | 4 bytes |

这里不展开了。


### Fragmentation

这里有两个重要且可能重叠的案例需要支持Fragmentation：

1. args不适合单个帧的大消息；建立请求时未知总大小的消息，类似于HTTP的分块编码；
2. 单个帧的最大大小有意限制为64kb，以避免共享TCP连接上的行头阻塞。此大小限制允许来自其他消息的帧与较大消息的帧交织。限制大小也会降低对任何中间节点的缓冲要求，并且在某些实现上可能支持固定大小分配的使用

所有的"call req"/"call res"在checksum之前，arg1， arg2和arg3必须都在单个帧中

发送这些字段后，checksum args被发送，或者大多数args都在这个帧中。如果这个帧小于64kb，这个帧是完整的，并且“more fragments follow”的flag不会被设置。然而，如果这个帧的空间不够以适应checksum和args，那么这个消息会分成多个子请求"call req continue". 并在最后一个子请求上, "more fragments follow"的flag不会被设置，这个消息是完整的

可能一开始给定args还不能确定这个是否要进行分片操作，帧中字段长度表示存在多少个args。当超过该arg结尾的帧中有更多数据时，arg被认为是完整的。如果arg碰巧在精确的帧边界上结束，则下一个连续帧将以该arg的0字节大小开始

Checksums是对帧的内容计算的，这个内容是args数据。每个帧的checksum来自先前帧的累积校验和seed。这允许在将arg有效负载传递给应用程序之前检测到损坏。

## Example

发送一个"call req", 带有4个字节的arg1，2个字节的arg2和8个字节的arg3. 这个args以极小的片段形成流，在这个分片例子中是很容易理解的。如果一个生产者进行流式更新，这可能在现实世界发生这种情况。如果预先知道总发送大小，则优选发送最大帧。

帧1是"call req"(0x03)带有"more fragments remain"的flag设置。这个ttl, tracing, traceflags, service 和headers都设置为合理的值。我们有2个字节已达到arg1，但是还有差2个字节。我们计算带有种子0的arg1 2个字节checksum，我们指定2个字节的长度，这个帧最后2个字节的数据流。这个接收解析这注意到这个arg1是不完整的。

帧2是一个"call req continue"(0x13)带有"more fragments remain"的flag设置，填充另外2个字节的arg1现在可用了。且arg2两个字节已填充。这个checksum会计算这个帧的arg1和arg2使用前阵帧的种子0xBEEF. 这个接收解析这知道，它需要继续arg1. Arg1在这个帧中有两个字节，但是这个帧仍然需要更多的比特流，因此这个接收解析器知道arg1现在是完整的。这个帧的结尾处还有arg2的2个字节。接受解析器知道arg2是不完整的，甚至我们知道在arg2中没有更多的比特流了。这样做是为了说明当arg在精确的框架边界上结束时会发生什么。

帧3是一个"call req continue"(0x23)，这个"more fragments remain"的flag被清除了。arg2是0比特，arg3是8个字节。这个checksum是以arg3的8个字节和前面帧的0xDEAD作为种子计算的。这个接收解析器知道，arg2是0个字节，表示arg2传输完成了。arg3是8个字节，且相应的flag被清除了，说明这个帧已经是最后一个了，那么arg3的数据也传输完成了。

| type | id | payload | 解析后的状态 |
| --- | --- | --- | --- |
| 0x03 | 1 | flags: 1=0x01, ttl 4:=0x2328, tracing:24 = 0x1,0x2,0x3, traceflags: 1= 0x1, service1 = 0x5 "svcA", nh: 1=0x1, hk1=0x1"k",  hv1=0xA"abcdefghij", csumtype:1=0x2 csum:4=0xBEEF arg12=0x2<2 bytes> | sending arg1 |
| 0x13 | 1 | flags:1=0x1, csumtype:1=0x2 csum:4=0xDEAD arg12=0x2<2 bytes> arg22=0x2<2 bytes> | sending arg2 |
| 0x23 | 1 | flags:1=0x0, csumtype:1=0x2 csum:4=0xF00F arg22=0x0<0 bytes> arg32=0x8<8 bytes>	| complete |

## Streaming

后面有时间在翻译::TODO

## 参考资料

[tchannel official docs](https://github.com/uber/tchannel/blob/master/docs)
